# Comparing Standard Recurrent Neural Network and Long Short-Term Memory Neural Network on Char-RNN

**Author:** Maya Que

## Abstract

This study investigates the efficacy of standard Recurrent Neural Networks (RNNs) versus Long Short-Term Memory (LSTM) networks through the Char-RNN model. A dataset comprising of texts from the Sherlock Holmes series was used, and two sets of Char-RNN models were trained using either a standard RNN or LSTM architecture. The comparative analysis focused on average loss across three trials over 20,000 training iterations for each omodel. Furthermore, the quality of generated text was evaluated to assess the model performance. Results show that LSTM netowrks signifcantly enhances learning efficiency compared to standard RNNs, with the text being generated by LSTM networks showing superior quality.


## Introduction

The Char-RNN model, as discussed by Karpathy in "The Unreasonable Effectiveness of Recurrent Neural Networks" [1], presents an effective approach to language model by operating at the character level to generate coherent and contextually relevant text. Unlike traditional word level models, Char-RNN processes text at the granularity of individual characters, which allows it to capture more fine-grained patterns and nuances and generate text with high degree of flexiblity.  Char-RNN is implemented using a recurrent neural network (RNN), a class of artifical neural netowrks for modelling sequential data. Whereas standard neural networks assumes fixed length input and output, RNN processes input sequences of variable lengths by maintaning a hidden state that retains information from previos time steps. This recurrent nature gives RNNs the ability to capture temporal dependencies and long range contextual information, making it suitable for tasks involved sequential data like language modelling. Figure 1 shows examples of how RNN operates over sequences of vectors in the input, output, or both. 

<iframe src="assets/diags.png" width=800 height=600 frameBorder=0></iframe>
Figure 1. Sequences of Vectors [1]


<iframe src="assets/rnn.png" width=800 height=600 frameBorder=0></iframe>
Figure 2. A Standard RNN 


The structure of a standard recurrent neural network can be represented through mathetmatical equations. On the forward pass, the hidden state $h^{(t)}$ at each step  $t$ is calculated using the following equation:

  $$h^{(t)} = \sigma(W^{hx}x^{(t)} + W^{hh}h^{(t-1)} + b_{h)}$$
  
Here, $x^{(t)}$ represents the input at time step $t$, $W^{hx}$ represents the weight matrix connecting the input to the hidden layer, $W^{hh}$ represents the weights for the recurrent connections, $b_{h}$ is the bias term, and $\sigma$ represents the activation function, commonly the sigmoid or hyperbolic tangent function. The hidden state $h^{(t)}$ is computed by combining information from the current input $x^{(t)}$ with the previous hidden state $h^{(t-1)}$, which enables the network to capture temporal dependencies and context.

Furthermore, the prediction $\hat y^{(t)}$ at time t is given by the application of a softmax funciton to the weighted sum of the current hidden state: 

  $$\hat y^{(t)} = softmax(W^{yh}h{(t)} + b_y)$$ 
  
In this equation, $W^{yh}$ denotes the weight matrix connectting the hidden state to the output layer, and $b_y$ represents the bias term. tHE hidden state is computed using both the input $x^{(t)}$ and the previous hidden state $h^{(t-1)}$, which allow recurrent neural networks to make predictions based on sequential information.

However, despite their efficiacy, traditional RNNs suffer from certain limitations such as difficulty in learning and retaining information over sequences of considerable length where information from disant itme steps become effectively inaccessible. This issue, commonly referred to as the "vanishing gradient" problem, arises due to the diminishing impact of gradients during backpropagation, which causes gradients to diminish exponentially over time, hindering the netowrk's ability to propoagrate information and learn long term dependencies. To address these challenges, Long Short Term Memory (LSTM) nettworks are introduced. LSTM netowrks offer a solution to the vanishing gradient problem by incorportating specialized memeory cells that are capable of retaining information over multiple time steps. Through a system of gated units that enables them to store and retrieve information over multiple time steps, LSTM networks selectively update and forget information over extended time horizons, facilitating more robust and efficient learning of long-range dependencies.

<iframe src="assets/lstm.png" width=800 height=600 frameBorder=0></iframe>
Figure 3. A LSTM (Full Network) 

Alhough LSTM network stands out for its ability to capture long term dependencies in sequential data, the effectiviess of LSTMs compared to standard RNN in tasks like text generation can be further investigated. This study aims to examine and compare the performance of standard RNN models with LSTM models within the context of the Char-RNN framework. Experiments will be conducted to assess the effectivenss of both architectures in terms of learning efficiency and etx geneation quality. Through comparing the strenghts and weaknesses of each model, thist study aims to contribute o a deeper understanding of their applicability in language modelling tasks. 

## Method
The dataset used comprises the complete works of Sherlock Holmes by Conan Doyle. After removing nonprintable characters, the daaset consisted of 3,381,831 characters. It also includes 100 distinct characters, including numerical digits, lowercase and uppercase letters, and special characters. 

The basic implementaion of the model was taken from the skeleton code provided in Homework 5 RNN. wo variants of recurrent neura networks, namely the standard RNN and Long Short-Term Memory (LSTM) networks were employed in this study. The models were implemented using RNNCell and LSTMCell classes available in the PyTorch torch.nn pakcage. Both models utilized a cross entropy loss function to compute training loss.

Three independent trials, each involving a distinct network, were conducted for both the standard RNN and LSTM architecture. Each trial consisted of 20,000 training iterations. During traiing, losses were recorded at intervals of 100 iterations. he recorded losses were subsequently averaged across the three trials for each model. This averaging process was then used to obtain representative measures of training loss. The primary evaluation metric employed in this study was the raining loss, which servedas an indicator of the model's learning efficiency.  Additioanlly, the quality of the text generated by each model was assessed qualitatively to gauge the effectiveness of the generated output. 

## Experiment

The experiment aimed to compare the performance of standard Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) networks within the context of the Char-RNN model. Specifically, the study sought to investigate whether the LSTM architecture, known for its ability to mitigate the vanishing gradient problem inherent in RNNs, would yield superior performance in terms of learning efficiency and text generation quality.

The null hypothesis posited that there would be no significant difference between the training loss curves for the RNN model and the LSTM model. However, based on the known challenges of vanishing gradients in RNNs and the more sophisticated design of LSTM networks, it was hypothesized that the LSTM model would outperform the standard RNN model in terms of learning efficiency and quality of generated text.

The experiment's results revealed a significant difference in the training loss curves between the RNN and LSTM models. The LSTM model exhibited a faster rate of decrease in average loss compared to the RNN model. Additionally, qualitative analysis of the generated text demonstrated that the LSTM model produced text with fewer errors and more coherent structure compared to the RNN model. These findings support the hypothesis that the LSTM model would outperform the standard RNN model in terms of learning efficiency and text generation quality.

To validate the observed improvements, the experiment extended training for the superior model (LSTM) for an additional 30,000 iterations (totaling 50,000 iterations). The generated text from this extended training period exhibited further improvement in coherence and semblance to the original Sherlock Holmes style, reinforcing the efficacy of the LSTM architecture for text generation tasks.

## Conclusion

This study shows the comparative perofrmance of stanadrd Recurrent Neural Networks (RNN) with Long Short-Term Meory (LSTM) netowroks within the Char-RNN model framework. 

A significant difference in the triannig loss curves was observed between the RNN and LSTM models over 20,000 training iterations. The LSTM architecture exhibited a notably faster rate of decrease in average loss compared to the standard RNN model. This underscores the effectiveness of LSTM networks in addressing the challenges associated with vanishing gradients in traditional RNNs, leading to more efficient learning. Qualiitative assesment of the etct geneated by the model furhter supported the quantitative findings. The LSTM model produced text with fewer errros, more coherent structure, and greater resemblance to the original style of Sherlock Holmes text, demosntrating its superior performance in text generation tasks. In conrtast, the text geneated by the standard RNN model exhibited more spelling errors,  non-existent words, and ireggular punctuation usage. Overall, these findings support the hypothesis that LSTM netowrks outperform standard RNNs in the context of Char-RNN for language modelling tasks. 

However, several limitations should also be acknowledged. This study focused on a specific dataset, so the findings may not be generalizable to other domains and applications. Additionally, due to limited computational resources, only three trials were conducted with 20,000 iterations for each trials. Having more trials will help make a more stastically conclusive finding about the perofrmance of these two models. The experiment also did not explore a fully comprehensive range of hyperparameters or model configurations. Other implementations of this study can do so to see how they influence the perofrmance of the archietceture.

This study can provide valuable insights for he field of natural language processing and neural mnetwork desing. The observed improvements of LSTM networks over standard RNNs in terms of learning effieicny and text generation quality highlights the importance of using architecture that is capable of addressing long range dependencies in sequntial data. Future research can explore further enhancements to LSTM-based models, such as incorporating attention mechanisms or exploring different cell architectures to optimize performance. By identifying the relative strengths and weaknesses of these models, this study conrtibutes to laying the groundwork for fuure advacenments in neural networkbased language modelling.

## Reference
[1] Karpathy, A. (2015). The Unreasonable Effectiveness of Recurrent Neural Networks. 
   Andrej Karpathy blog. http://karpathy.github.io/2015/05/21/rnn-effectiveness/
   
[2] Doyle, C. The complete Sherlock Holmes. 
   https://sherlock-holm.es/stories/plain-text/cnus.txt

[3] Tu, Z. (2024, March 22). Lecture 14: Recurrent Neural Networks [Lecture notes].
   COGS 181, Winter 2023 Neural Networks and Deep Learning, University of California, San Diego.

[5] Tu, Z. (2024). Homework Assignment 5 RNN.
   COGS 181, Winter 2023 Neural Networks and Deep Learning, University of California, San Diego.